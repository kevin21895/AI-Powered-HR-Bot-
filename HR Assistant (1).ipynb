{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "088d96ef-aec8-40a5-ad8f-9708d4953cec",
   "metadata": {},
   "source": [
    "# AI-Powered HR Assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086338dd-bb33-4f8d-8456-6f21cd8d95a1",
   "metadata": {},
   "source": [
    "## This project creates a user friendly chatbot which can answer Nestle's employees HR queries  using PDF Document. \n",
    "\n",
    "Tools used - <br>\n",
    "    1. LangChain <br>\n",
    "    2. Open AI GPT-3.5 Turbo Model <br>\n",
    "    3. Gradio <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91c4d418-3675-4f1d-8227-055cc9b6a72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "import os\n",
    "import openai\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain import PromptTemplate\n",
    "import gradio as gr\n",
    "from datetime import datetime, timedelta\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c9700ae-0f3a-454a-afbd-398d292b31c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load PDF file using LangChain\n",
    "def load_pdf(filename):\n",
    "    pdf_loader = PyPDFLoader(filename)\n",
    "    extracted_text = pdf_loader.load()\n",
    "    return extracted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31dcfa8f-3294-4d90-b031-30df75b6b66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split text from documents into chunks\n",
    "def recursive_charcter_text_splitter(extracted_text,chunk_size,chunk_overlap,seperator = [\" \"]):\n",
    "    text_splitter  = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=seperator\n",
    "    )\n",
    "    splitted_text=text_splitter.split_documents(extracted_text)\n",
    "    return splitted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f47efbd-d662-47bc-bc0c-e1441a2c39a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Embeddings using FAISS\n",
    "def create_vectordb_faiss(splitted_text,embeddings):\n",
    "    vectordb = FAISS.from_documents(\n",
    "    documents=splitted_text,\n",
    "    embedding=embeddings)\n",
    "    return vectordb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4def557d-7e5a-4658-a589-10abf002c1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an instance of Open AI model\n",
    "def create_openai_model(model_name,temperature = 0):\n",
    "    llm = ChatOpenAI(model_name=model_name, temperature=temperature)\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c80fc1aa-5100-42dd-b08e-8695ecfca07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a chain to retreive Relavant documents based on prompts\n",
    "def create_retreiver_chain(vectordb,llm,return_source_documents = False,chain_type_kwargs = None):\n",
    "    retreiver_a = vectordb.as_retriever()\n",
    "    retriever_chain = RetrievalQA.from_chain_type(llm,\n",
    "                                           retriever =retreiver_a,\n",
    "                                           return_source_documents=return_source_documents,\n",
    "                                           chain_type_kwargs = chain_type_kwargs\n",
    "                                           )\n",
    "    return retriever_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc83ecb8-9b64-4f32-8e98-0b8965839b18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filename = \"hr_policy.pdf\"\n",
    "model_name = \"gpt-3.5-turbo\"\n",
    "extracted_text = load_pdf(filename)\n",
    "splitted_text = recursive_charcter_text_splitter(extracted_text,1024,256,[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d90ff8eb-ac30-4c65-ba86-74ee87760a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "vector_db = create_vectordb_faiss(splitted_text,embeddings)\n",
    "llm = create_openai_model(model_name,0)\n",
    "#retriever_chain = create_retreiver_chain(vector_db,llm,True,{\"prompt\": prompt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d31d898c-7e4f-4067-a41b-4ba2667a887a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a prompt to answer all the questions in Professsional tone using PDF as context\n",
    "style = \"Gujarati\"\n",
    "prompt_template = \"\"\"You are an assistant for question-answering tasks.\n",
    "Answer in a style that is \"\"\" + style + \"\"\"\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=['question'])\n",
    "retriever_chain = create_retreiver_chain(vector_db,llm,True,{\"prompt\": prompt})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c22d9aa-105e-4a15-b75b-9b1962e8c0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65/4169608285.py:12: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(placeholder=\"<strong>Welcome Nestle's HR BOT</strong><br>Ask Me Anything\")\n",
      "/voc/work/.local/lib/python3.10/site-packages/gradio/chat_interface.py:315: UserWarning: The type of the gr.Chatbot does not match the type of the gr.ChatInterface.The type of the gr.ChatInterface, 'messages', will be used.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://a1e0e54db6ed8363fa.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://a1e0e54db6ed8363fa.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Creating function to take the questions and get the answer using GPT Model\n",
    "def run_llm(message,history):\n",
    "    result = retriever_chain(message)\n",
    "    response = result['result']\n",
    "    return response\n",
    "\n",
    "#Creating a chat interface using Gradio\n",
    "demo = gr.ChatInterface(run_llm, type=\"messages\", autofocus=True,\n",
    "    flagging_mode=\"manual\",\n",
    "    flagging_options=[\"Like\", \"Spam\", \"Inappropriate\", \"Other\"],\n",
    "    save_history=True,\n",
    "    chatbot = gr.Chatbot(placeholder=\"<strong>Welcome Nestle's HR BOT</strong><br>Ask Me Anything\")\n",
    "                       )\n",
    "\n",
    "#Chat Interface will automatically close in 10 minutes\n",
    "start_time = datetime.now()\n",
    "ten_mins = 0\n",
    "demo.launch(share = True)\n",
    "while ten_mins == 0:\n",
    "    now_time = datetime.now()\n",
    "    time_difference = now_time - start_time\n",
    "    nine_minutes = timedelta(minutes=10)\n",
    "    if time_difference >= nine_minutes:\n",
    "        demo.close()\n",
    "        ten_mins = 1\n",
    "    else:\n",
    "        time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0e2059-eb06-4cb4-b0c2-21376f93c5eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 [3.10]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
